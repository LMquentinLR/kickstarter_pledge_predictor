{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline on Kickstarter Pledge Dataset\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1. Instructions\n",
    "\n",
    "- **Choosing any sufficiently large open dataset** (less than 100000 lines are not allowed)\n",
    "\n",
    "\n",
    "- **Choosing one variable to predict**\n",
    "\n",
    "\n",
    "- **Implementing at least two supervised learning models**: classification, regression, recommender system, etc. Unsupervised tasks (e.g. clusterisation, associative rules, etc.) are not allowed\n",
    "\n",
    "\n",
    "- **Mandatory use of Apache Spark** (e.g. on Google Cloud as we did during our lab sessions)\n",
    "\n",
    "\n",
    "- A **full machine learning pipeline must be implemented**, which include:\n",
    "    - Reading the data\n",
    "    - Transforming data (extracting features, dealing with missing values if any, etc.)\n",
    "    - Building models (build at least two models to compare)\n",
    "    - Evaluating quality (use cross-validation or train/test split)\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "### 1.3. Summary & Conclusion\n",
    "\n",
    "The notebook was also ran locally using the installation steps for Spark described [here](https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/).\n",
    "\n",
    "## 2. Environment Set-Up\n",
    "\n",
    "We need the following libraries installed to set up the environment:\n",
    "\n",
    "- kaggle (see documentation [here](https://github.com/Kaggle/kaggle-api#datasets))\n",
    "- pyspark (see documentation [here](https://spark.apache.org/docs/latest/api/python/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/qlr/anaconda3/lib/python3.8/site-packages (1.5.10)\n",
      "Requirement already satisfied: python-dateutil in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: certifi in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.6.20)\n",
      "Requirement already satisfied: tqdm in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.50.2)\n",
      "Requirement already satisfied: six>=1.10 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: requests in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.24.0)\n",
      "Requirement already satisfied: python-slugify in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: urllib3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: pyspark in /home/qlr/anaconda3/lib/python3.8/site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/qlr/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes existing files that may have been downloaded locally\n",
    "!rm -f kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading kickstarter-projects.zip to /home/qlr/Programming/kickstarter_pledge_prediction\n",
      " 98%|█████████████████████████████████████▏| 36.0M/36.8M [00:03<00:00, 9.53MB/s]\n",
      "100%|██████████████████████████████████████| 36.8M/36.8M [00:04<00:00, 9.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Dowloads the raw dataset from the kaggle source\n",
    "!kaggle datasets download -d kemical/kickstarter-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kickstarter-projects.zip\n",
      "  inflating: ks-projects-201612.csv  \n",
      "  inflating: ks-projects-201801.csv  \n",
      "draft.ipynb\t\tREADME.md\t\t      spark_pipeline_vDef.ipynb\n",
      "ks-projects-201801.csv\tspark-ml-full-pipeline.ipynb  Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Unzips the raw dataset and keeps only the most recent instance\n",
    "!unzip kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv kickstarter-projects.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep 'ks-projects-201801.csv', the most recent dataset available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Library Imports & Spark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import unix_timestamp, ceil, isnan, when, count, col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ks-projects-201801.csv\"\n",
    "dataset_format = \"csv\"\n",
    "spark_context = \"local\" #if run on a local computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Starting a Spark Session & Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(spark_context)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://desktop-lkaf740-2.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f06e43b2100>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = (spark\n",
    "             .read\n",
    "             .format(dataset_format)\n",
    "             .options(header=True)\n",
    "             .load(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_to_keep = [\n",
    "    \"ID\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "replace_start_end_dates_with_duration = [\n",
    "    \"ID\",\"name\",\"category\",\"total_duration\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_decision_tree = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "deadline_format = \"yyyy-MM-dd\"\n",
    "launched_format = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('main_category', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('launched', 'string'),\n",
       " ('pledged', 'string'),\n",
       " ('state', 'string'),\n",
       " ('backers', 'string'),\n",
       " ('country', 'string'),\n",
       " ('usd pledged', 'string'),\n",
       " ('usd_pledged_real', 'string'),\n",
       " ('usd_goal_real', 'string')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks the type of each columns\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "| ID|name|category|main_category|currency|deadline|goal|launched|pledged|state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "|  0|   0|       0|            0|       0|       0|   0|       0|      0|    0|      0|      0|          0|               0|            0|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checks for N/A\n",
    "campaigns.select([count(when(isnan(c), c)).alias(c) for c in campaigns.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 378661 rows.\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "|        ID|                name|      category|  deadline|           launched|country|usd_goal_real|   state|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "|1000002330|The Songs of Adel...|        Poetry|2015-10-09|2015-08-11 12:12:28|     GB|      1533.95|  failed|\n",
      "|1000003930|Greeting From Ear...|Narrative Film|2017-11-01|2017-09-02 04:43:57|     US|     30000.00|  failed|\n",
      "|1000004038|      Where is Hank?|Narrative Film|2013-02-26|2013-01-12 00:20:50|     US|     45000.00|  failed|\n",
      "|1000007540|ToshiCapital Reko...|         Music|2012-04-16|2012-03-17 03:24:11|     US|      5000.00|  failed|\n",
      "|1000011046|Community Film Pr...|  Film & Video|2015-08-29|2015-07-04 08:35:03|     US|     19500.00|canceled|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the relevant columns\n",
    "campaigns = campaigns.select(raw_columns_to_keep)\n",
    "# --\n",
    "print(f\"The dataset contains {campaigns.count()} rows.\")\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "|        ID|                name|      category|total_duration|country|usd_goal_real|   state|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "|1000002330|The Songs of Adel...|        Poetry|            59|     GB|      1533.95|  failed|\n",
      "|1000003930|Greeting From Ear...|Narrative Film|            60|     US|     30000.00|  failed|\n",
      "|1000004038|      Where is Hank?|Narrative Film|            45|     US|     45000.00|  failed|\n",
      "|1000007540|ToshiCapital Reko...|         Music|            30|     US|      5000.00|  failed|\n",
      "|1000011046|Community Film Pr...|  Film & Video|            56|     US|     19500.00|canceled|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes a duration time (in day) from the launch and deadline features before dropping them\n",
    "launch_times = unix_timestamp('launched', format = launched_format)\n",
    "deadline_times = unix_timestamp('deadline', format = deadline_format)\n",
    "time_difference = deadline_times - launch_times\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"total_duration\",ceil(time_difference/(3600*24))).\\\n",
    "    select(replace_start_end_dates_with_duration)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casts the relevant column(s) to their end types\n",
    "for column in [\"total_duration\", \"usd_goal_real\"]:\n",
    "    campaigns = campaigns.withColumn(column,col(column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Building a dataset for a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 378661 rows.\n",
      "+--------------+-------------+--------------+-------+--------+\n",
      "|total_duration|usd_goal_real|      category|country|   state|\n",
      "+--------------+-------------+--------------+-------+--------+\n",
      "|          59.0|      1533.95|        Poetry|     GB|  failed|\n",
      "|          60.0|      30000.0|Narrative Film|     US|  failed|\n",
      "|          45.0|      45000.0|Narrative Film|     US|  failed|\n",
      "|          30.0|       5000.0|         Music|     US|  failed|\n",
      "|          56.0|      19500.0|  Film & Video|     US|canceled|\n",
      "+--------------+-------------+--------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('total_duration', 'float'),\n",
       " ('country', 'string'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID and name from the dataset\n",
    "decision_tree_dataset = campaigns.select(kept_columns_for_decision_tree)\n",
    "# --\n",
    "print(f\"The dataset contains {decision_tree_dataset.count()} rows.\")\n",
    "decision_tree_dataset.show(n=5)\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running a DecisionTreeClassifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Creating a data processing pipeline\n",
    "\n",
    "We will rely on indexing and assembling our data pipeline using the following stages:\n",
    "- **StringIndexer** for all categorical columns\n",
    "- **OneHotEncoder** for all categorical index columns\n",
    "- **VectorAssembler** for all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.8\n",
    "validation_size = 0.2\n",
    "max_depth_grid = list(range(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to string index each categorical feature column, and the label column\n",
    "categorical_feature_columns = decision_tree_dataset.columns[2:4]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to one-hot encode each categorical feature column\n",
    "onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column,\n",
    "                                                 handleInvalid=\"keep\") \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                        outputCol='features',\n",
    "                                        handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_model = data_processing_pipeline.fit(decision_tree_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 377364 rows.\n",
      "+-----------------+---------------+--------------+-------------+--------------------+-----+\n",
      "|  onehot_category| onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+-----------------+---------------+--------------+-------------+--------------------+-----+\n",
      "|(1441,[62],[1.0])|(226,[1],[1.0])|          59.0|      1533.95|(1669,[62,1442,16...|  0.0|\n",
      "|(1441,[22],[1.0])|(226,[0],[1.0])|          60.0|      30000.0|(1669,[22,1441,16...|  0.0|\n",
      "|(1441,[22],[1.0])|(226,[0],[1.0])|          45.0|      45000.0|(1669,[22,1441,16...|  0.0|\n",
      "| (1441,[2],[1.0])|(226,[0],[1.0])|          30.0|       5000.0|(1669,[2,1441,166...|  0.0|\n",
      "| (1441,[7],[1.0])|(226,[0],[1.0])|          56.0|      19500.0|(1669,[7,1441,166...|  2.0|\n",
      "+-----------------+---------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = processed_feature_columns + ['features', 'label']\n",
    "decision_tree_dataset_prepped = pipeline_model.transform(decision_tree_dataset).select(final_columns)\n",
    "# --\n",
    "print(f\"The dataset contains {decision_tree_dataset_prepped.count()} rows.\")        \n",
    "decision_tree_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, validation = decision_tree_dataset_prepped.randomSplit([training_size, validation_size], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Creating a model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, max_depth_grid).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(decision_tree_dataset_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"features\"),\n",
    "    LinearRegression(labelCol=\"stars\")\n",
    "])\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\"regParam\", [0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "####\n",
    "\n",
    "debug_data = data.sample(1.0)\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "models = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"stars\")\n",
    ").fit(debug_data)\n",
    "\n",
    "####\n",
    "\n",
    "models.validationMetrics\n",
    "\n",
    "####\n",
    "\n",
    "models.getEvaluator().getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
