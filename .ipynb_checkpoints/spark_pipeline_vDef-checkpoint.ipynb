{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline on Kickstarter Pledge Dataset\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1. Instructions\n",
    "\n",
    "- **Choosing any sufficiently large open dataset** (less than 100000 lines are not allowed)\n",
    "\n",
    "\n",
    "- **Choosing one variable to predict**\n",
    "\n",
    "\n",
    "- **Implementing at least two supervised learning models**: classification, regression, recommender system, etc. Unsupervised tasks (e.g. clusterisation, associative rules, etc.) are not allowed\n",
    "\n",
    "\n",
    "- **Mandatory use of Apache Spark** (e.g. on Google Cloud as we did during our lab sessions)\n",
    "\n",
    "\n",
    "- A **full machine learning pipeline must be implemented**, which include:\n",
    "    - Reading the data\n",
    "    - Transforming data (extracting features, dealing with missing values if any, etc.)\n",
    "    - Building models (build at least two models to compare)\n",
    "    - Evaluating quality (use cross-validation or train/test split)\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "### 1.3. Summary & Conclusion\n",
    "\n",
    "The notebook was also ran locally using the installation steps for Spark described [here](https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/).\n",
    "\n",
    "## 2. Environment Set-Up\n",
    "\n",
    "We need the following libraries installed to set up the environment:\n",
    "\n",
    "- kaggle (see documentation [here](https://github.com/Kaggle/kaggle-api#datasets))\n",
    "- pyspark (see documentation [here](https://spark.apache.org/docs/latest/api/python/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/qlr/anaconda3/lib/python3.8/site-packages (1.5.10)\n",
      "Requirement already satisfied: python-dateutil in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: certifi in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.6.20)\n",
      "Requirement already satisfied: urllib3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: requests in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.50.2)\n",
      "Requirement already satisfied: python-slugify in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: pyspark in /home/qlr/anaconda3/lib/python3.8/site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/qlr/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes existing files that may have been downloaded locally\n",
    "!rm -f kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**On Google Cloud**: To download the kaggle dataset we have to upload our kaggle.json file in /root/.kaggle. the kaggle.json file can be downloaded here:</span>\n",
    "\n",
    "> ``https://www.kaggle.com/<username>/account``\n",
    " \n",
    "<span style=\"color:red\">**Locally**: To download the kaggle dataset we have to upload our kaggle.json file in ~/.kaggle.</span>\n",
    "    \n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Moves the kaggle.json file from your user directory to the root folder\n",
    "# when using Google Cloud\n",
    "!mkdir /home/qlr\n",
    "!mv home/qlr/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/qlr/.kaggle/kaggle.json'\n",
      "Downloading kickstarter-projects.zip to /home/qlr/Programming/kickstarter_pledge_prediction\n",
      "100%|██████████████████████████████████████| 36.8M/36.8M [00:04<00:00, 8.53MB/s]\n",
      "100%|██████████████████████████████████████| 36.8M/36.8M [00:04<00:00, 9.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Dowloads the raw dataset from the kaggle source\n",
    "!kaggle datasets download -d kemical/kickstarter-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kickstarter-projects.zip\n",
      "  inflating: ks-projects-201612.csv  \n",
      "  inflating: ks-projects-201801.csv  \n",
      "draft_run1_DataProc.ipynb  README.md\n",
      "ks-projects-201801.csv\t   spark_pipeline_vDef.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Unzips the raw dataset and keeps only the most recent instance\n",
    "!unzip kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv kickstarter-projects.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Zips the raw dataset once instantiated in memory\n",
    "# Useful for git push as Github gives a warning when pushing a file > 50Mb\n",
    "!zip ks-projects-201801.zip ks-projects-201801.csv\n",
    "!rm -f ks-projects-201801.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep 'ks-projects-201801.csv', the most recent dataset available.\n",
    "\n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Uploads the dataset to HDFS when on Google Cloud\n",
    "!hdfs dfs -mkdir /user/qlr\n",
    "!hdfs dfs -put ks-projects-201801.csv /user/qlr\n",
    "!hdfs dfs -ls /user/qlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Library Imports & Spark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import unix_timestamp, ceil, isnan, when, count, col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ks-projects-201801.csv\" #if run on a local computer\n",
    "#dataset_path = \"/user/qlr/ks-projects-201801.csv\" # if run on Google Cloud\n",
    "dataset_format = \"csv\"\n",
    "spark_context = \"local\" #if run on a local computer\n",
    "#spark_context = \"yarn\" #if run on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Starting a Spark Session & Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Comment out the following cell when when running the notebook Google Cloud as a spark session is automatically instantiated.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(spark_context)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://desktop-lkaf740-2.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff89136cd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #Spark UI on Google Cloud should return v2.3.4 (version), yarn (Master), PySparkShell (AppName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = (spark\n",
    "             .read\n",
    "             .format(dataset_format)\n",
    "             .options(header=True)\n",
    "             .load(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_to_keep = [\n",
    "    \"ID\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "replace_start_end_dates_with_duration = [\n",
    "    \"ID\",\"name\",\"category\",\"total_duration\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_logistic_regression = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"name\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_decision_tree = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "deadline_format = \"yyyy-MM-dd\"\n",
    "launched_format = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('main_category', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('launched', 'string'),\n",
       " ('pledged', 'string'),\n",
       " ('state', 'string'),\n",
       " ('backers', 'string'),\n",
       " ('country', 'string'),\n",
       " ('usd pledged', 'string'),\n",
       " ('usd_pledged_real', 'string'),\n",
       " ('usd_goal_real', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks the type of each columns\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "| ID|name|category|main_category|currency|deadline|goal|launched|pledged|state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "|  0|   0|       0|            0|       0|       0|   0|       0|      0|    0|      0|      0|          0|               0|            0|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drops NAs, Nulls, and Duplicates as pySpark can raise an error during .fit() procedures\n",
    "campaigns = campaigns.dropna()\n",
    "campaigns = campaigns.dropDuplicates()\n",
    "for column in campaigns.columns:\n",
    "    campaigns = campaigns.where(col(column).isNotNull())\n",
    "\n",
    "# Checks for N/A\n",
    "campaigns.select([count(when(isnan(c), c)).alias(c) for c in campaigns.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 374855 rows.\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "|        ID|                name|      category|  deadline|           launched|country|usd_goal_real|     state|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "|  10018239|             Borders|         Drama|2016-04-08|2016-02-25 17:40:34|     GB|      4926.39|    failed|\n",
      "|1002250421|Spiele für iOS un...|  Mobile Games|2015-06-24|2015-06-03 17:12:38|     DE|      2240.39|  canceled|\n",
      "|1002289150|Odyssey Skateboar...|Graphic Design|2014-12-20|2014-11-20 06:55:12|     US|       700.00|    failed|\n",
      "|1005414218|Debut EP Album Pr...|           R&B|2014-11-26|2014-10-27 17:46:28|     US|      5500.00|    failed|\n",
      "|1005696636|GBS Detroit Prese...|    Indie Rock|2013-06-08|2013-05-23 21:05:24|     US|      1200.00|successful|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the relevant columns\n",
    "campaigns = campaigns.select(raw_columns_to_keep)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(campaigns.count()) + \" rows.\")\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "|        ID|                name|      category|total_duration|country|usd_goal_real|     state|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "|  10018239|             Borders|         Drama|            43|     GB|      4926.39|    failed|\n",
      "|1002250421|Spiele für iOS un...|  Mobile Games|            21|     DE|      2240.39|  canceled|\n",
      "|1002289150|Odyssey Skateboar...|Graphic Design|            30|     US|       700.00|    failed|\n",
      "|1005414218|Debut EP Album Pr...|           R&B|            30|     US|      5500.00|    failed|\n",
      "|1005696636|GBS Detroit Prese...|    Indie Rock|            16|     US|      1200.00|successful|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes a duration time (in day) from the launch and deadline features before dropping them\n",
    "launch_times = unix_timestamp('launched', format = launched_format)\n",
    "deadline_times = unix_timestamp('deadline', format = deadline_format)\n",
    "time_difference = deadline_times - launch_times\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"total_duration\",ceil(time_difference/(3600*24))).\\\n",
    "    select(replace_start_end_dates_with_duration)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     state| count|\n",
      "+----------+------+\n",
      "|    failed|237451|\n",
      "|successful|134609|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleans the target labels\n",
    "# 'undefied', 'live' -> dropped\n",
    "# 'suspended', 'cancelled' -> renamed to 'failed'\n",
    "for condition in ['state!=\"undefined\"', 'state!=\"live\"']:\n",
    "    campaigns = campaigns.where(condition)\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"state\",when(col(\"state\") == \"canceled\", \"failed\").\\\n",
    "    when(col(\"state\") == \"suspended\", \"failed\").\\\n",
    "    when(col(\"state\") == \"failed\", \"failed\").\\\n",
    "    otherwise(\"successful\"))\n",
    "campaigns.select(\"state\").groupBy('state').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casts the relevant column(s) to their end types\n",
    "for column in [\"total_duration\", \"usd_goal_real\"]:\n",
    "    campaigns = campaigns.withColumn(column,col(column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Finalizing our dataset for a logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 372060 rows.\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "|total_duration|usd_goal_real|                name|      category|country|     state|\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "|          43.0|      4926.39|             Borders|         Drama|     GB|    failed|\n",
      "|          21.0|      2240.39|Spiele für iOS un...|  Mobile Games|     DE|    failed|\n",
      "|          30.0|        700.0|Odyssey Skateboar...|Graphic Design|     US|    failed|\n",
      "|          30.0|       5500.0|Debut EP Album Pr...|           R&B|     US|    failed|\n",
      "|          16.0|       1200.0|GBS Detroit Prese...|    Indie Rock|     US|successful|\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('total_duration', 'float'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('country', 'string'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID from the dataset\n",
    "logistic_regression_dataset = campaigns.select(kept_columns_for_logistic_regression)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(logistic_regression_dataset.count()) + \" rows.\")\n",
    "logistic_regression_dataset.show(n=5)\n",
    "logistic_regression_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Finalizing our dataset for a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 372060 rows.\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "|total_duration|usd_goal_real|      category|country|     state|\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "|          43.0|      4926.39|         Drama|     GB|    failed|\n",
      "|          21.0|      2240.39|  Mobile Games|     DE|    failed|\n",
      "|          30.0|        700.0|Graphic Design|     US|    failed|\n",
      "|          30.0|       5500.0|           R&B|     US|    failed|\n",
      "|          16.0|       1200.0|    Indie Rock|     US|successful|\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('total_duration', 'float'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('category', 'string'),\n",
       " ('country', 'string'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID and name from the dataset\n",
    "decision_tree_dataset = campaigns.select(kept_columns_for_decision_tree)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(decision_tree_dataset.count()) + \" rows.\")\n",
    "decision_tree_dataset.show(n=5)\n",
    "decision_tree_dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running a Logistic Regression Pipeline\n",
    "\n",
    "### 7.1. Creating a data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.7\n",
    "test = 0.3\n",
    "\n",
    "def process_confusion_matrix(matrix):\n",
    "    items = []\n",
    "    for item in matrix: \n",
    "        items.append(item)\n",
    "        print(item, matrix[item])\n",
    "    if Row(label=0.0, prediction=0.0) in items: \n",
    "        true_negatives = matrix[Row(label=0.0, prediction=0.0)]\n",
    "    else: \n",
    "        true_negatives = 0\n",
    "    if Row(label=1.0, prediction=0.0) in items: \n",
    "        false_negatives = matrix[Row(label=1.0, prediction=0.0)]\n",
    "    else: \n",
    "        false_negatives = 0\n",
    "    if Row(label=0.0, prediction=1.0) in items: \n",
    "        false_positives = matrix[Row(label=0.0, prediction=1.0)]\n",
    "    else: \n",
    "        false_positives = 0\n",
    "    if Row(label=1.0, prediction=1.0) in items: \n",
    "        true_positives = matrix[Row(label=1.0, prediction=1.0)]\n",
    "    else: \n",
    "        true_positives = 0\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    print(\"\\nPrecision score:\", precision)\n",
    "    print(\"Recall score:\", recall)\n",
    "    print(\"F1 score:\", (precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to string index each categorical feature column except name, and the label column\n",
    "categorical_feature_columns = decision_tree_dataset.columns[3:]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to one-hot encode each categorical feature column except name\n",
    "if spark_context == \"local\":\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                     outputCol='onehot_' + column,\n",
    "                                                  handleInvalid=\"keep\") \n",
    "                                      for column in categorical_feature_columns]\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column) \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column except name\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "\n",
    "if spark_context == \"local\":\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features_1',\n",
    "                                            handleInvalid=\"skip\")\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_features = VectorAssembler(inputCols=[\"features_1\", \"features_2\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage] + \n",
    "    [tokenizer] + \n",
    "    [hashingTF] +\n",
    "    [merge_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_model = data_processing_pipeline.fit(logistic_regression_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(262373,[1,225,22...|  0.0|\n",
      "|(262373,[4,225,22...|  0.0|\n",
      "|(262373,[0,225,22...|  0.0|\n",
      "|(262373,[0,225,22...|  0.0|\n",
      "|(262373,[0,226,22...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final_columns = processed_feature_columns + ['features_1', 'features_2', 'features', 'label']\n",
    "final_columns = ['features', 'label']\n",
    "logistic_regression_dataset_prepped = pipeline_model.transform(logistic_regression_dataset).select(final_columns)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(logistic_regression_dataset_prepped.count()) + \" rows.\")        \n",
    "logistic_regression_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Creating a model pipeline\n",
    "\n",
    "#### 7.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, test = logistic_regression_dataset_prepped.randomSplit([training_size, test], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "model = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: \n",
      "{Param(parent='LogisticRegression_72be9f1cc593', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_72be9f1cc593', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_72be9f1cc593', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_72be9f1cc593', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_72be9f1cc593', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_72be9f1cc593', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_72be9f1cc593', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='LogisticRegression_72be9f1cc593', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_72be9f1cc593', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_72be9f1cc593', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_72be9f1cc593', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent='LogisticRegression_72be9f1cc593', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_72be9f1cc593', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_72be9f1cc593', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, prediction=0.0) 166066\n",
      "Row(label=1.0, prediction=1.0) 93405\n",
      "\n",
      "Precision score: 1.0\n",
      "Recall score: 1.0\n",
      "F1 score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = model.transform(training).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, prediction=0.0) 71383\n",
      "Row(label=1.0, prediction=1.0) 39917\n",
      "Row(label=1.0, prediction=0.0) 2\n",
      "Row(label=0.0, prediction=1.0) 2\n",
      "\n",
      "Precision score: 0.9999498985445527\n",
      "Recall score: 0.9999498985445527\n",
      "F1 score: 0.49997494927227637\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = model.transform(test).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running a DecisionTreeClassifier Pipeline\n",
    "\n",
    "### 8.1. Creating a data processing pipeline\n",
    "\n",
    "We will rely on indexing and assembling our data pipeline using the following stages:\n",
    "- **StringIndexer** for all categorical columns\n",
    "- **OneHotEncoder** for all categorical index columns\n",
    "- **VectorAssembler** for all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.7\n",
    "test = 0.3\n",
    "max_depth_grid = list(range(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to string index each categorical feature column, and the label column\n",
    "categorical_feature_columns = decision_tree_dataset.columns[2:4]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to one-hot encode each categorical feature column\n",
    "if spark_context == \"local\":\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                     outputCol='onehot_' + column,\n",
    "                                                  handleInvalid=\"keep\") \n",
    "                                      for column in categorical_feature_columns]\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column) \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "\n",
    "if spark_context == \"local\":\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid=\"skip\")\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_model = data_processing_pipeline.fit(decision_tree_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "|   onehot_category| onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "| (1429,[49],[1.0])|(225,[1],[1.0])|          43.0|      4926.39|(1656,[49,1430,16...|  0.0|\n",
      "| (1429,[57],[1.0])|(225,[4],[1.0])|          21.0|      2240.39|(1656,[57,1433,16...|  0.0|\n",
      "| (1429,[52],[1.0])|(225,[0],[1.0])|          30.0|        700.0|(1656,[52,1429,16...|  0.0|\n",
      "|(1429,[107],[1.0])|(225,[0],[1.0])|          30.0|       5500.0|(1656,[107,1429,1...|  0.0|\n",
      "| (1429,[20],[1.0])|(225,[0],[1.0])|          16.0|       1200.0|(1656,[20,1429,16...|  1.0|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = processed_feature_columns + ['features', 'label']\n",
    "decision_tree_dataset_prepped = pipeline_model.transform(decision_tree_dataset).select(final_columns)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(decision_tree_dataset_prepped.count()) + \" rows.\")        \n",
    "decision_tree_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 8.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_crossvalidation = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(decision_tree_with_crossvalidation.maxDepth, max_depth_grid).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=decision_tree_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(decision_tree_dataset_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|            features|label|prediction|    rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|(1656,[49,1430,16...|  0.0|       0.0|  [9046.0,4711.0]|[0.65755615323108...|\n",
      "|(1656,[57,1433,16...|  0.0|       1.0|[16940.0,20443.0]|[0.45314715244897...|\n",
      "|(1656,[52,1429,16...|  0.0|       0.0|[63112.0,43818.0]|[0.59021789956046...|\n",
      "|(1656,[107,1429,1...|  0.0|       0.0|[36474.0,15691.0]|[0.69920444742643...|\n",
      "|(1656,[20,1429,16...|  1.0|       1.0|[16940.0,20443.0]|[0.45314715244897...|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(decision_tree_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, prediction=0.0) 215444\n",
      "Row(label=0.0, prediction=1.0) 22007\n",
      "Row(label=1.0, prediction=1.0) 31696\n",
      "Row(label=1.0, prediction=0.0) 101628\n",
      "\n",
      "Precision score: 0.5902091130849301\n",
      "Recall score: 0.23773664156490953\n",
      "F1 score: 0.1694728568602394\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(decision_tree_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best MaxDepth is: 7\n"
     ]
    }
   ],
   "source": [
    "print('The best MaxDepth is:', cv_model.bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fea654ee5622, depth=7, numNodes=141, numClasses=2, numFeatures=1656\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Creating a model pipeline using Train-Test split\n",
    "\n",
    "#### 8.3.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, test = decision_tree_dataset_prepped.randomSplit([training_size, test], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_traintestsplit = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "traintest_model = decision_tree_with_traintestsplit.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|            features|label|prediction|    rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[11983.0,14731.0]|[0.44856629482668...|\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[11983.0,14731.0]|[0.44856629482668...|\n",
      "|(1656,[0,1429,165...|  1.0|       0.0|[76301.0,24377.0]|[0.75787163034625...|\n",
      "|(1656,[0,1429,165...|  0.0|       0.0|[29037.0,15753.0]|[0.64829202947086...|\n",
      "|(1656,[0,1429,165...|  0.0|       0.0|[29037.0,15753.0]|[0.64829202947086...|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_test = traintest_model.transform(test)\n",
    "pred_test.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error is 0.53302975805233\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(\"The test error is\", 1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_3a42e543f85a, depth=5, numNodes=21, numClasses=2, numFeatures=1656\n"
     ]
    }
   ],
   "source": [
    "print(traintest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running a Naive Bayes Pipeline\n",
    "\n",
    "### 9.1. Creating a data processing pipeline\n",
    "\n",
    "We will be reusing the decision tree classifier's pipeline and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.7\n",
    "test = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "|   onehot_category| onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "| (1429,[49],[1.0])|(225,[1],[1.0])|          43.0|      4926.39|(1656,[49,1430,16...|  0.0|\n",
      "| (1429,[57],[1.0])|(225,[4],[1.0])|          21.0|      2240.39|(1656,[57,1433,16...|  0.0|\n",
      "| (1429,[52],[1.0])|(225,[0],[1.0])|          30.0|        700.0|(1656,[52,1429,16...|  0.0|\n",
      "|(1429,[107],[1.0])|(225,[0],[1.0])|          30.0|       5500.0|(1656,[107,1429,1...|  0.0|\n",
      "| (1429,[20],[1.0])|(225,[0],[1.0])|          16.0|       1200.0|(1656,[20,1429,16...|  1.0|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fits the data processing pipeline\n",
    "naive_bayes_dataset_prepped = data_processing_pipeline.\\\n",
    "    fit(decision_tree_dataset).\\\n",
    "    transform(decision_tree_dataset).\\\n",
    "    select(final_columns)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(naive_bayes_dataset_prepped.count()) + \" rows.\")        \n",
    "naive_bayes_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 9.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Builds the estimator\n",
    "naive_bayes_with_crossvalidation = NaiveBayes(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(naive_bayes_with_crossvalidation.smoothing, list(range(0,10))).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=naive_bayes_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(naive_bayes_dataset_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|            features|label|prediction|       rawPrediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|(1656,[49,1430,16...|  0.0|       1.0|[-356.60990544225...|[7.05060917583924...|\n",
      "|(1656,[57,1433,16...|  0.0|       1.0|[-191.24644420456...|[7.28721749948837...|\n",
      "|(1656,[52,1429,16...|  0.0|       1.0|[-254.30340586274...|[4.14940891015285...|\n",
      "|(1656,[107,1429,1...|  0.0|       1.0|[-258.31128577039...|[1.43917923843405...|\n",
      "|(1656,[20,1429,16...|  1.0|       1.0|[-148.76538831054...|[8.18699192732424...|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(naive_bayes_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, prediction=1.0) 174849\n",
      "Row(label=1.0, prediction=1.0) 118816\n",
      "Row(label=0.0, prediction=0.0) 62602\n",
      "Row(label=1.0, prediction=0.0) 14508\n",
      "\n",
      "Precision score: 0.40459707489826846\n",
      "Recall score: 0.8911823827667936\n",
      "F1 score: 0.2782647796547452\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(decision_tree_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameter smoothing has best value: 9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The parameter smoothing has best value:\", cv_model.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel: uid=NaiveBayes_bc6353a34b7d, modelType=multinomial, numClasses=2, numFeatures=1656\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"features\"),\n",
    "    LinearRegression(labelCol=\"stars\")\n",
    "])\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\"regParam\", [0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "####\n",
    "\n",
    "debug_data = data.sample(1.0)\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "models = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"stars\")\n",
    ").fit(debug_data)\n",
    "\n",
    "####\n",
    "\n",
    "models.validationMetrics\n",
    "\n",
    "####\n",
    "\n",
    "models.getEvaluator().getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
