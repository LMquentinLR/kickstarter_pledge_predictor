{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline on Kickstarter Pledge Dataset\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1. Instructions\n",
    "\n",
    "- **Choosing any sufficiently large open dataset** (less than 100000 lines are not allowed)\n",
    "\n",
    "\n",
    "- **Choosing one variable to predict**\n",
    "\n",
    "\n",
    "- **Implementing at least two supervised learning models**: classification, regression, recommender system, etc. Unsupervised tasks (e.g. clusterisation, associative rules, etc.) are not allowed\n",
    "\n",
    "\n",
    "- **Mandatory use of Apache Spark** (e.g. on Google Cloud as we did during our lab sessions)\n",
    "\n",
    "\n",
    "- A **full machine learning pipeline must be implemented**, which include:\n",
    "    - Reading the data\n",
    "    - Transforming data (extracting features, dealing with missing values if any, etc.)\n",
    "    - Building models (build at least two models to compare)\n",
    "    - Evaluating quality (use cross-validation or train/test split)\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "### 1.3. Summary & Conclusion\n",
    "\n",
    "The notebook was also ran locally using the installation steps for Spark described [here](https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/).\n",
    "\n",
    "## 2. Environment Set-Up\n",
    "\n",
    "We need the following libraries installed to set up the environment:\n",
    "\n",
    "- kaggle (see documentation [here](https://github.com/Kaggle/kaggle-api#datasets))\n",
    "- pyspark (see documentation [here](https://spark.apache.org/docs/latest/api/python/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/qlr/anaconda3/lib/python3.8/site-packages (1.5.10)\n",
      "Requirement already satisfied: python-dateutil in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: urllib3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: tqdm in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (4.50.2)\n",
      "Requirement already satisfied: requests in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2.24.0)\n",
      "Requirement already satisfied: certifi in /home/qlr/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.6.20)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/qlr/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/qlr/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: pyspark in /home/qlr/anaconda3/lib/python3.8/site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/qlr/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes existing files that may have been downloaded locally\n",
    "!rm -f kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**On Google Cloud**: To download the kaggle dataset we have to upload our kaggle.json file in /root/.kaggle. the kaggle.json file can be downloaded here:</span>\n",
    "\n",
    "> ``https://www.kaggle.com/<username>/account``\n",
    " \n",
    "<span style=\"color:red\">**Locally**: To download the kaggle dataset we have to upload our kaggle.json file in ~/.kaggle.</span>\n",
    "    \n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Moves the kaggle.json file from your user directory to the root folder\n",
    "# when using Google Cloud\n",
    "!mkdir /home/qlr\n",
    "!mv home/qlr/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/qlr/.kaggle/kaggle.json'\n",
      "Downloading kickstarter-projects.zip to /home/qlr/Programming/kickstarter_pledge_prediction\n",
      " 98%|█████████████████████████████████████▏| 36.0M/36.8M [00:03<00:00, 7.85MB/s]\n",
      "100%|██████████████████████████████████████| 36.8M/36.8M [00:04<00:00, 9.64MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Dowloads the raw dataset from the kaggle source\n",
    "!kaggle datasets download -d kemical/kickstarter-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kickstarter-projects.zip\n",
      "  inflating: ks-projects-201612.csv  \n",
      "  inflating: ks-projects-201801.csv  \n",
      "draft_run1_DataProc.ipynb  README.md\n",
      "ks-projects-201801.csv\t   spark_pipeline_vDef.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Unzips the raw dataset and keeps only the most recent instance\n",
    "!unzip kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv kickstarter-projects.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Zips the raw dataset once instantiated in memory\n",
    "# Useful for git push as Github gives a warning when pushing a file > 50Mb\n",
    "!zip ks-projects-201801.zip ks-projects-201801.csv\n",
    "!rm -f ks-projects-201801.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep 'ks-projects-201801.csv', the most recent dataset available.\n",
    "\n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Uploads the dataset to HDFS when on Google Cloud\n",
    "!hdfs dfs -mkdir /user/qlr\n",
    "!hdfs dfs -put ks-projects-201801.csv /user/qlr\n",
    "!hdfs dfs -ls /user/qlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Library Imports & Spark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import unix_timestamp, ceil, isnan, when, count, col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ks-projects-201801.csv\" #if run on a local computer\n",
    "#dataset_path = \"/user/qlr/ks-projects-201801.csv\" # if run on Google Cloud\n",
    "dataset_format = \"csv\"\n",
    "spark_context = \"local\" #if run on a local computer\n",
    "#spark_context = \"yarn\" #if run on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Starting a Spark Session & Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Comment out the following cell when when running the notebook Google Cloud as a spark session is automatically instantiated.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(spark_context)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://desktop-lkaf740-2.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2e2460e250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #Spark UI on Google Cloud should return v2.3.4 (version), yarn (Master), PySparkShell (AppName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = (spark\n",
    "             .read\n",
    "             .format(dataset_format)\n",
    "             .options(header=True)\n",
    "             .load(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_to_keep = [\n",
    "    \"ID\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "replace_start_end_dates_with_duration = [\n",
    "    \"ID\",\"name\",\"category\",\"total_duration\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_decision_tree = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "deadline_format = \"yyyy-MM-dd\"\n",
    "launched_format = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('main_category', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('launched', 'string'),\n",
       " ('pledged', 'string'),\n",
       " ('state', 'string'),\n",
       " ('backers', 'string'),\n",
       " ('country', 'string'),\n",
       " ('usd pledged', 'string'),\n",
       " ('usd_pledged_real', 'string'),\n",
       " ('usd_goal_real', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks the type of each columns\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "| ID|name|category|main_category|currency|deadline|goal|launched|pledged|state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "|  0|   0|       0|            0|       0|       0|   0|       0|      0|    0|      0|      0|          0|               0|            0|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drops NAs, Nulls, and Duplicates as pySpark can raise an error during .fit() procedures\n",
    "campaigns = campaigns.dropna()\n",
    "campaigns = campaigns.dropDuplicates()\n",
    "for column in campaigns.columns:\n",
    "    campaigns = campaigns.where(col(column).isNotNull())\n",
    "\n",
    "# Checks for N/A\n",
    "campaigns.select([count(when(isnan(c), c)).alias(c) for c in campaigns.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 374855 rows.\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "|        ID|                name|      category|  deadline|           launched|country|usd_goal_real|     state|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "|  10018239|             Borders|         Drama|2016-04-08|2016-02-25 17:40:34|     GB|      4926.39|    failed|\n",
      "|1002250421|Spiele für iOS un...|  Mobile Games|2015-06-24|2015-06-03 17:12:38|     DE|      2240.39|  canceled|\n",
      "|1002289150|Odyssey Skateboar...|Graphic Design|2014-12-20|2014-11-20 06:55:12|     US|       700.00|    failed|\n",
      "|1005414218|Debut EP Album Pr...|           R&B|2014-11-26|2014-10-27 17:46:28|     US|      5500.00|    failed|\n",
      "|1005696636|GBS Detroit Prese...|    Indie Rock|2013-06-08|2013-05-23 21:05:24|     US|      1200.00|successful|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the relevant columns\n",
    "campaigns = campaigns.select(raw_columns_to_keep)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(campaigns.count()) + \" rows.\")\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "|        ID|                name|      category|total_duration|country|usd_goal_real|     state|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "|  10018239|             Borders|         Drama|            43|     GB|      4926.39|    failed|\n",
      "|1002250421|Spiele für iOS un...|  Mobile Games|            21|     DE|      2240.39|  canceled|\n",
      "|1002289150|Odyssey Skateboar...|Graphic Design|            30|     US|       700.00|    failed|\n",
      "|1005414218|Debut EP Album Pr...|           R&B|            30|     US|      5500.00|    failed|\n",
      "|1005696636|GBS Detroit Prese...|    Indie Rock|            16|     US|      1200.00|successful|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes a duration time (in day) from the launch and deadline features before dropping them\n",
    "launch_times = unix_timestamp('launched', format = launched_format)\n",
    "deadline_times = unix_timestamp('deadline', format = deadline_format)\n",
    "time_difference = deadline_times - launch_times\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"total_duration\",ceil(time_difference/(3600*24))).\\\n",
    "    select(replace_start_end_dates_with_duration)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     state| count|\n",
      "+----------+------+\n",
      "|    failed|237451|\n",
      "|successful|134609|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleans the target labels\n",
    "# 'undefied', 'live' -> dropped\n",
    "# 'suspended', 'cancelled' -> renamed to 'failed'\n",
    "for condition in ['state!=\"undefined\"', 'state!=\"live\"']:\n",
    "    campaigns = campaigns.where(condition)\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"state\",when(col(\"state\") == \"canceled\", \"failed\").\\\n",
    "    when(col(\"state\") == \"suspended\", \"failed\").\\\n",
    "    when(col(\"state\") == \"failed\", \"failed\").\\\n",
    "    otherwise(\"successful\"))\n",
    "campaigns.select(\"state\").groupBy('state').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casts the relevant column(s) to their end types\n",
    "for column in [\"total_duration\", \"usd_goal_real\"]:\n",
    "    campaigns = campaigns.withColumn(column,col(column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Finalizing our dataset for a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 372060 rows.\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "|total_duration|usd_goal_real|      category|country|     state|\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "|          43.0|      4926.39|         Drama|     GB|    failed|\n",
      "|          21.0|      2240.39|  Mobile Games|     DE|    failed|\n",
      "|          30.0|        700.0|Graphic Design|     US|    failed|\n",
      "|          30.0|       5500.0|           R&B|     US|    failed|\n",
      "|          16.0|       1200.0|    Indie Rock|     US|successful|\n",
      "+--------------+-------------+--------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('total_duration', 'float'),\n",
       " ('country', 'string'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID and name from the dataset\n",
    "decision_tree_dataset = campaigns.select(kept_columns_for_decision_tree)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(decision_tree_dataset.count()) + \" rows.\")\n",
    "decision_tree_dataset.show(n=5)\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running a Logistic Regression Pipeline\n",
    "\n",
    "### 7.1. Creating a data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Creating a model pipeline\n",
    "\n",
    "#### 7.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running a DecisionTreeClassifier Pipeline\n",
    "\n",
    "### 8.1. Creating a data processing pipeline\n",
    "\n",
    "We will rely on indexing and assembling our data pipeline using the following stages:\n",
    "- **StringIndexer** for all categorical columns\n",
    "- **OneHotEncoder** for all categorical index columns\n",
    "- **VectorAssembler** for all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.7\n",
    "test = 0.3\n",
    "max_depth_grid = list(range(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to string index each categorical feature column, and the label column\n",
    "categorical_feature_columns = decision_tree_dataset.columns[2:4]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to one-hot encode each categorical feature column\n",
    "if spark_context == \"local\":\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                     outputCol='onehot_' + column,\n",
    "                                                  handleInvalid=\"keep\") \n",
    "                                      for column in categorical_feature_columns]\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column) \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "\n",
    "if spark_context == \"local\":\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features',\n",
    "                                            handleInvalid=\"skip\")\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_model = data_processing_pipeline.fit(decision_tree_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "|   onehot_category| onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "| (1429,[49],[1.0])|(225,[1],[1.0])|          43.0|      4926.39|(1656,[49,1430,16...|  0.0|\n",
      "| (1429,[57],[1.0])|(225,[4],[1.0])|          21.0|      2240.39|(1656,[57,1433,16...|  0.0|\n",
      "| (1429,[52],[1.0])|(225,[0],[1.0])|          30.0|        700.0|(1656,[52,1429,16...|  0.0|\n",
      "|(1429,[107],[1.0])|(225,[0],[1.0])|          30.0|       5500.0|(1656,[107,1429,1...|  0.0|\n",
      "| (1429,[20],[1.0])|(225,[0],[1.0])|          16.0|       1200.0|(1656,[20,1429,16...|  1.0|\n",
      "+------------------+---------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = processed_feature_columns + ['features', 'label']\n",
    "decision_tree_dataset_prepped = pipeline_model.transform(decision_tree_dataset).select(final_columns)\n",
    "# --\n",
    "print(f\"The dataset contains \" + str(decision_tree_dataset_prepped.count()) + \" rows.\")        \n",
    "decision_tree_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 8.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_crossvalidation = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, max_depth_grid).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=decision_tree_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(decision_tree_dataset_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|            features|label|prediction|    rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[16269.0,19840.0]|[0.45055249383810...|\n",
      "|(1656,[0,1429,165...|  0.0|       0.0| [22014.0,9358.0]|[0.70170852989927...|\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[16269.0,19840.0]|[0.45055249383810...|\n",
      "|(1656,[0,1429,165...|  1.0|       0.0| [22014.0,9358.0]|[0.70170852989927...|\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[16269.0,19840.0]|[0.45055249383810...|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(decision_tree_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(decision_tree_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, prediction=0.0) 210815\n",
      "Row(label=0.0, prediction=1.0) 26636\n",
      "Row(label=1.0, prediction=1.0) 37338\n",
      "Row(label=1.0, prediction=0.0) 95986\n",
      "\n",
      "Precision score: 0.5836433551130147\n",
      "Recall score: 0.28005460382226754\n",
      "F1 score: 0.18924672323084876\n"
     ]
    }
   ],
   "source": [
    "def process_confusion_matrix(matrix):\n",
    "    items = []\n",
    "    for item in matrix: \n",
    "        items.append(item)\n",
    "        print(item, matrix[item])\n",
    "    true_positives = matrix[items[2]]\n",
    "    true_negatives = matrix[items[0]]\n",
    "    false_positives = matrix[items[1]]\n",
    "    false_negatives = matrix[items[3]]\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    print(\"\\nPrecision score:\", precision)\n",
    "    print(\"Recall score:\", recall)\n",
    "    print(\"F1 score:\", (precision*recall)/(precision+recall))\n",
    "    \n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best MaxDepth is: 8\n"
     ]
    }
   ],
   "source": [
    "print('The best MaxDepth is:', cv_model.bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a85fc1ee317d, depth=8, numNodes=291, numClasses=2, numFeatures=1656\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Creating a model pipeline using Train-Test split\n",
    "\n",
    "#### 8.3.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'DataFrame' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-07f2c9ef7bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Splits the dataset between training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_dataset_prepped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrandomSplit\u001b[0;34m(self, weights, seed)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \"\"\"\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights must be positive. Found weight value: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'DataFrame' and 'float'"
     ]
    }
   ],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, test = decision_tree_dataset_prepped.randomSplit([training_size, test], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_traintestsplit = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "traintest_model = decision_tree_with_traintestsplit.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|            features|label|prediction|    rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[11984.0,14733.0]|[0.44855335554141...|\n",
      "|(1656,[0,1429,165...|  1.0|       1.0|[11984.0,14733.0]|[0.44855335554141...|\n",
      "|(1656,[0,1429,165...|  1.0|       0.0|[76302.0,24381.0]|[0.75784392598551...|\n",
      "|(1656,[0,1429,165...|  0.0|       0.0|[29035.0,15749.0]|[0.64833422650946...|\n",
      "|(1656,[0,1429,165...|  0.0|       0.0|[29035.0,15749.0]|[0.64833422650946...|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_test = traintest_model.transform(test)\n",
    "pred_test.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error is 0.629766864951931\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(\"The test error is\", 1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_8e97afc23f0e, depth=5, numNodes=21, numClasses=2, numFeatures=1656\n"
     ]
    }
   ],
   "source": [
    "print(traintest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"features\"),\n",
    "    LinearRegression(labelCol=\"stars\")\n",
    "])\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\"regParam\", [0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "####\n",
    "\n",
    "debug_data = data.sample(1.0)\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "models = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"stars\")\n",
    ").fit(debug_data)\n",
    "\n",
    "####\n",
    "\n",
    "models.validationMetrics\n",
    "\n",
    "####\n",
    "\n",
    "models.getEvaluator().getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
