{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline on Kickstarter Pledge Dataset\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1. Instructions\n",
    "\n",
    "- **Choosing any sufficiently large open dataset** (less than 100000 lines are not allowed)\n",
    "\n",
    "\n",
    "- **Choosing one variable to predict**\n",
    "\n",
    "\n",
    "- **Implementing at least two supervised learning models**: classification, regression, recommender system, etc. Unsupervised tasks (e.g. clusterisation, associative rules, etc.) are not allowed\n",
    "\n",
    "\n",
    "- **Mandatory use of Apache Spark** (e.g. on Google Cloud as we did during our lab sessions)\n",
    "\n",
    "\n",
    "- A **full machine learning pipeline must be implemented**, which include:\n",
    "    - Reading the data\n",
    "    - Transforming data (extracting features, dealing with missing values if any, etc.)\n",
    "    - Building models (build at least two models to compare)\n",
    "    - Evaluating quality (use cross-validation or train/test split)\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "### 1.3. Summary & Conclusion\n",
    "\n",
    "The notebook was also ran locally using the installation steps for Spark described [here](https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/).\n",
    "\n",
    "## 2. Environment Set-Up\n",
    "\n",
    "We need the following libraries installed to set up the environment:\n",
    "\n",
    "- kaggle (see documentation [here](https://github.com/Kaggle/kaggle-api#datasets))\n",
    "- pyspark (see documentation [here](https://spark.apache.org/docs/latest/api/python/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes existing files that may have been downloaded locally\n",
    "!rm -f kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls\n",
    "!mv home/qlr/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowloads the raw dataset from the kaggle source\n",
    "!kaggle datasets download -d kemical/kickstarter-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzips the raw dataset and keeps only the most recent instance\n",
    "!unzip kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv kickstarter-projects.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Zips the raw dataset once instantiated in memory\n",
    "# Useful for git push as Github gives a warning when pushing a file > 50Mb\n",
    "!zip ks-projects-201801.zip ks-projects-201801.csv\n",
    "!rm -f ks-projects-201801.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep 'ks-projects-201801.csv', the most recent dataset available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Library Imports & Spark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import unix_timestamp, ceil, isnan, when, count, col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/user/qlr/ks-projects-201801.csv\"\n",
    "dataset_format = \"csv\"\n",
    "#spark_context = \"local\" #if run on a local computer\n",
    "spark_context = \"\" #if run on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Starting a Spark Session & Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(spark_context)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-1805-m.europe-west1-d.c.rising-timing-296017.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2a64936390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = (spark\n",
    "             .read\n",
    "             .format(dataset_format)\n",
    "             .options(header=True)\n",
    "             .load(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_to_keep = [\n",
    "    \"ID\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "replace_start_end_dates_with_duration = [\n",
    "    \"ID\",\"name\",\"category\",\"total_duration\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_decision_tree = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "deadline_format = \"yyyy-MM-dd\"\n",
    "launched_format = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('main_category', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('launched', 'string'),\n",
       " ('pledged', 'string'),\n",
       " ('state', 'string'),\n",
       " ('backers', 'string'),\n",
       " ('country', 'string'),\n",
       " ('usd pledged', 'string'),\n",
       " ('usd_pledged_real', 'string'),\n",
       " ('usd_goal_real', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks the type of each columns\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "| ID|name|category|main_category|currency|deadline|goal|launched|pledged|state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "|  0|   0|       0|            0|       0|       0|   0|       0|      0|    0|      0|      0|          0|               0|            0|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checks for N/A\n",
    "campaigns.select([count(when(isnan(c), c)).alias(c) for c in campaigns.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "|        ID|                name|      category|  deadline|           launched|country|usd_goal_real|   state|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "|1000002330|The Songs of Adel...|        Poetry|2015-10-09|2015-08-11 12:12:28|     GB|      1533.95|  failed|\n",
      "|1000003930|Greeting From Ear...|Narrative Film|2017-11-01|2017-09-02 04:43:57|     US|     30000.00|  failed|\n",
      "|1000004038|      Where is Hank?|Narrative Film|2013-02-26|2013-01-12 00:20:50|     US|     45000.00|  failed|\n",
      "|1000007540|ToshiCapital Reko...|         Music|2012-04-16|2012-03-17 03:24:11|     US|      5000.00|  failed|\n",
      "|1000011046|Community Film Pr...|  Film & Video|2015-08-29|2015-07-04 08:35:03|     US|     19500.00|canceled|\n",
      "+----------+--------------------+--------------+----------+-------------------+-------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the relevant columns\n",
    "campaigns = campaigns.select(raw_columns_to_keep)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "|        ID|                name|      category|total_duration|country|usd_goal_real|   state|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "|1000002330|The Songs of Adel...|        Poetry|            59|     GB|      1533.95|  failed|\n",
      "|1000003930|Greeting From Ear...|Narrative Film|            60|     US|     30000.00|  failed|\n",
      "|1000004038|      Where is Hank?|Narrative Film|            45|     US|     45000.00|  failed|\n",
      "|1000007540|ToshiCapital Reko...|         Music|            30|     US|      5000.00|  failed|\n",
      "|1000011046|Community Film Pr...|  Film & Video|            56|     US|     19500.00|canceled|\n",
      "+----------+--------------------+--------------+--------------+-------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes a duration time (in day) from the launch and deadline features before dropping them\n",
    "launch_times = unix_timestamp('launched', format = launched_format)\n",
    "deadline_times = unix_timestamp('deadline', format = deadline_format)\n",
    "time_difference = deadline_times - launch_times\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"total_duration\",ceil(time_difference/(3600*24))).\\\n",
    "    select(replace_start_end_dates_with_duration)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = campaigns.dropna()\n",
    "for column in campaigns.columns:\n",
    "    campaigns = campaigns.where(col(column).isNotNull())\n",
    "campaigns = campaigns.dropDuplicates()\n",
    "for condition in ['state!=\"undefined\"', 'state!=\"live\"']:\n",
    "    campaigns = campaigns.where(condition)\n",
    "campaigns.select(\"state\").groupBy('state').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     state| count|\n",
      "+----------+------+\n",
      "|    failed|237579|\n",
      "|successful|133429|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "campaigns = campaigns.dropna()\n",
    "for column in campaigns.columns:\n",
    "    campaigns = campaigns.where(col(column).isNotNull())\n",
    "campaigns = campaigns.dropDuplicates()\n",
    "for condition in ['state!=\"undefined\"', 'state!=\"live\"']:\n",
    "    campaigns = campaigns.where(condition)\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"state\",when(col(\"state\") == \"canceled\", \"failed\").\\\n",
    "    when(col(\"state\") == \"suspended\", \"failed\").\\\n",
    "    when(col(\"state\") == \"failed\", \"failed\").\\\n",
    "    otherwise(\"successful\"))\n",
    "campaigns.select(\"state\").groupBy('state').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371008"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaigns.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casts the relevant column(s) to their end types\n",
    "for column in [\"total_duration\", \"usd_goal_real\"]:\n",
    "    campaigns = campaigns.withColumn(column,col(column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Building a dataset for a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------+-------+----------+\n",
      "|total_duration|usd_goal_real|   category|country|     state|\n",
      "+--------------+-------------+-----------+-------+----------+\n",
      "|          29.0|       5000.0|Periodicals|     US|successful|\n",
      "|          17.0|        300.0|  Sculpture|     US|successful|\n",
      "|          30.0|      60000.0|Documentary|     US|    failed|\n",
      "|          30.0|     222200.0| Television|     US|    failed|\n",
      "|          30.0|      10000.0|        Pop|     US|successful|\n",
      "+--------------+-------------+-----------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('total_duration', 'float'),\n",
       " ('country', 'string'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID and name from the dataset\n",
    "decision_tree_dataset = campaigns.select(kept_columns_for_decision_tree)\n",
    "# --\n",
    "decision_tree_dataset.show(n=5)\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running a DecisionTreeClassifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Creating a data processing pipeline\n",
    "\n",
    "We will rely on indexing and assembling our data pipeline using the following stages:\n",
    "- **StringIndexer** for all categorical columns\n",
    "- **OneHotEncoder** for all categorical index columns\n",
    "- **VectorAssembler** for all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.8\n",
    "validation_size = 0.2\n",
    "max_depth_grid = list(range(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to string index each categorical feature column, and the label column\n",
    "categorical_feature_columns = decision_tree_dataset.columns[2:4]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to one-hot encode each categorical feature column\n",
    "onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column) \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                        outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_model = data_processing_pipeline.fit(decision_tree_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "| onehot_category|onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "|(158,[65],[1.0])|(22,[0],[1.0])|          29.0|       5000.0|(182,[65,158,180,...|  1.0|\n",
      "|(158,[55],[1.0])|(22,[0],[1.0])|          17.0|        300.0|(182,[55,158,180,...|  1.0|\n",
      "| (158,[1],[1.0])|(22,[0],[1.0])|          30.0|      60000.0|(182,[1,158,180,1...|  0.0|\n",
      "|(158,[75],[1.0])|(22,[0],[1.0])|          30.0|     222200.0|(182,[75,158,180,...|  0.0|\n",
      "|(158,[30],[1.0])|(22,[0],[1.0])|          30.0|      10000.0|(182,[30,158,180,...|  1.0|\n",
      "+----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = processed_feature_columns + ['features', 'label']\n",
    "decision_tree_dataset_prepped = pipeline_model.transform(decision_tree_dataset.na.drop()).select(final_columns)\n",
    "# --       \n",
    "decision_tree_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "|  onehot_category|onehot_country|total_duration|usd_goal_real|            features|label|\n",
      "+-----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "|  (158,[4],[1.0])|(22,[0],[1.0])|          30.0|       6500.0|(182,[4,158,180,1...|  1.0|\n",
      "|  (158,[3],[1.0])|(22,[0],[1.0])|          40.0|       1200.0|(182,[3,158,180,1...|  0.0|\n",
      "|(158,[101],[1.0])|(22,[0],[1.0])|          25.0|        400.0|(182,[101,158,180...|  1.0|\n",
      "|  (158,[2],[1.0])|(22,[0],[1.0])|           9.0|       3219.0|(182,[2,158,180,1...|  1.0|\n",
      "| (158,[12],[1.0])|(22,[2],[1.0])|          45.0|      3701.24|(182,[12,160,180,...|  1.0|\n",
      "|  (158,[4],[1.0])|(22,[0],[1.0])|          30.0|       7500.0|(182,[4,158,180,1...|  1.0|\n",
      "| (158,[50],[1.0])|(22,[1],[1.0])|          30.0|       166.33|(182,[50,159,180,...|  0.0|\n",
      "|  (158,[0],[1.0])|(22,[0],[1.0])|          27.0|       1999.0|(182,[0,158,180,1...|  1.0|\n",
      "|  (158,[0],[1.0])|(22,[0],[1.0])|          30.0|      25000.0|(182,[0,158,180,1...|  0.0|\n",
      "|  (158,[3],[1.0])|(22,[0],[1.0])|          31.0|       8000.0|(182,[3,158,180,1...|  1.0|\n",
      "| (158,[10],[1.0])|(22,[0],[1.0])|          30.0|       1000.0|(182,[10,158,180,...|  0.0|\n",
      "|  (158,[2],[1.0])|(22,[0],[1.0])|          28.0|       2500.0|(182,[2,158,180,1...|  0.0|\n",
      "|  (158,[6],[1.0])|(22,[0],[1.0])|          34.0|      10000.0|(182,[6,158,180,1...|  0.0|\n",
      "|  (158,[8],[1.0])|(22,[0],[1.0])|          30.0|      20000.0|(182,[8,158,180,1...|  0.0|\n",
      "| (158,[23],[1.0])|(22,[2],[1.0])|          30.0|     22355.36|(182,[23,160,180,...|  0.0|\n",
      "|  (158,[9],[1.0])|(22,[0],[1.0])|          30.0|      10000.0|(182,[9,158,180,1...|  0.0|\n",
      "| (158,[27],[1.0])|(22,[1],[1.0])|          30.0|       389.34|(182,[27,159,180,...|  0.0|\n",
      "| (158,[10],[1.0])|(22,[0],[1.0])|          30.0|       2500.0|(182,[10,158,180,...|  1.0|\n",
      "|  (158,[4],[1.0])|(22,[1],[1.0])|          41.0|      6868.22|(182,[4,159,180,1...|  0.0|\n",
      "|  (158,[0],[1.0])|(22,[0],[1.0])|          40.0|      40000.0|(182,[0,158,180,1...|  0.0|\n",
      "+-----------------+--------------+--------------+-------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checks for N/A\n",
    "decision_tree_dataset_prepped.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, validation = decision_tree_dataset_prepped.randomSplit([training_size, validation_size], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Creating a model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, max_depth_grid).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(decision_tree_dataset_prepped.na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|            features|label|prediction|    rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "|(182,[0,158,180,1...|  0.0|       1.0|[24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  1.0|       1.0|[24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  0.0|       1.0|[24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  0.0|       0.0|[69694.0,28701.0]|[0.70830834900147...|\n",
      "|(182,[0,158,180,1...|  1.0|       1.0|[24479.0,28377.0]|[0.46312622975631...|\n",
      "+--------------------+-----+----------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(training)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+------------------+--------------------+\n",
      "|            features|label|prediction|     rawPrediction|         probability|\n",
      "+--------------------+-----+----------+------------------+--------------------+\n",
      "|(182,[0,158,180,1...|  1.0|       1.0| [24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  0.0|       1.0| [24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  1.0|       1.0| [24479.0,28377.0]|[0.46312622975631...|\n",
      "|(182,[0,158,180,1...|  0.0|       0.0|[104275.0,70239.0]|[0.59751653162496...|\n",
      "|(182,[0,158,180,1...|  0.0|       0.0| [69694.0,28701.0]|[0.70830834900147...|\n",
      "+--------------------+-----+----------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on test data\n",
    "pred_test_cv = cv_model.transform(validation)\n",
    "pred_test_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(label=0.0, prediction=0.0): 213100,\n",
       "             Row(label=0.0, prediction=1.0): 24479,\n",
       "             Row(label=1.0, prediction=0.0): 105052,\n",
       "             Row(label=1.0, prediction=1.0): 28377})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(decision_tree_dataset_prepped).select('label', 'prediction')\n",
    "label_and_pred.rdd.zipWithIndex().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The best MaxDepth is:', 2)\n"
     ]
    }
   ],
   "source": [
    "print('The best MaxDepth is:', cv_model.bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 371008 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains \" + str(campaigns.count()) + \" rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(inputCol=\"text\", outputCol=\"words\"),\n",
    "    HashingTF(inputCol=\"words\", outputCol=\"term_frequency\"),\n",
    "    IDF(inputCol=\"term_frequency\", outputCol=\"features\"),\n",
    "    LinearRegression(labelCol=\"stars\")\n",
    "])\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(\"regParam\", [0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "####\n",
    "\n",
    "debug_data = data.sample(1.0)\n",
    "\n",
    "####\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "models = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"stars\")\n",
    ").fit(debug_data)\n",
    "\n",
    "####\n",
    "\n",
    "models.validationMetrics\n",
    "\n",
    "####\n",
    "\n",
    "models.getEvaluator().getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
