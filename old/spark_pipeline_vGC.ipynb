{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Pipeline on Kickstarter Pledge Dataset\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "### 1.1. Instructions\n",
    "\n",
    "- **Choosing any sufficiently large open dataset** (less than 100000 lines are not allowed)\n",
    "\n",
    "\n",
    "- **Choosing one variable to predict**\n",
    "\n",
    "\n",
    "- **Implementing at least two supervised learning models**: classification, regression, recommender system, etc. Unsupervised tasks (e.g. clusterisation, associative rules, etc.) are not allowed\n",
    "\n",
    "\n",
    "- **Mandatory use of Apache Spark** (e.g. on Google Cloud as we did during our lab sessions)\n",
    "\n",
    "\n",
    "- A **full machine learning pipeline must be implemented**, which include:\n",
    "    - Reading the data\n",
    "    - Transforming data (extracting features, dealing with missing values if any, etc.)\n",
    "    - Building models (build at least two models to compare)\n",
    "    - Evaluating quality (use cross-validation or train/test split)\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "### 1.3. Summary & Conclusion\n",
    "\n",
    "The notebook was also ran locally using the installation steps for Spark described [here](https://sparkbyexamples.com/spark/spark-installation-on-linux-ubuntu/).\n",
    "\n",
    "## 2. Environment Set-Up\n",
    "\n",
    "We need the following libraries installed to set up the environment:\n",
    "\n",
    "- kaggle (see documentation [here](https://github.com/Kaggle/kaggle-api#datasets))\n",
    "- pyspark (see documentation [here](https://spark.apache.org/docs/latest/api/python/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied: tqdm in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: requests in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: certifi in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/anaconda/lib/python2.7/site-packages (from kaggle)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/anaconda/lib/python2.7/site-packages (from python-slugify->kaggle)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/anaconda/lib/python2.7/site-packages (from requests->kaggle)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/anaconda/lib/python2.7/site-packages (from requests->kaggle)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyspark in /usr/lib/spark/python\n",
      "Requirement already satisfied: py4j==0.10.7 in /opt/conda/anaconda/lib/python2.7/site-packages (from pyspark)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes existing files that may have been downloaded locally\n",
    "!rm -f kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**On Google Cloud**: To download the kaggle dataset we have to upload our kaggle.json file in /root/.kaggle. the kaggle.json file can be downloaded here:</span>\n",
    "\n",
    "> ``https://www.kaggle.com/<username>/account``\n",
    " \n",
    "<span style=\"color:red\">**Locally**: To download the kaggle dataset we have to upload our kaggle.json file in ~/.kaggle.</span>\n",
    "    \n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud. **It is assumed we created a /home/user folder where we uploaded our JupyterNotebook and our kaggle.json file in Dataproc**</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Moves the kaggle.json file from your user directory to the root folder\n",
    "# when using Google Cloud\n",
    "\n",
    "!mv home/qlr/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading kickstarter-projects.zip to /\n",
      " 90%|██████████████████████████████████    | 33.0M/36.8M [00:00<00:00, 47.6MB/s]\n",
      "100%|██████████████████████████████████████| 36.8M/36.8M [00:00<00:00, 50.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Dowloads the raw dataset from the kaggle source\n",
    "!kaggle datasets download -d kemical/kickstarter-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kickstarter-projects.zip\n",
      "  inflating: ks-projects-201612.csv  \n",
      "  inflating: ks-projects-201801.csv  \n",
      "bin   etc     ks-projects-201801.csv  lib64\t  media  proc  sbin  tmp\n",
      "boot  hadoop  lib\t\t      libx32\t  mnt\t root  srv   usr\n",
      "dev   home    lib32\t\t      lost+found  opt\t run   sys   var\n"
     ]
    }
   ],
   "source": [
    "# Unzips the raw dataset and keeps only the most recent instance\n",
    "!unzip kickstarter-projects.zip\n",
    "!rm -f ks-projects-201612.csv kickstarter-projects.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Zips the raw dataset once instantiated in memory\n",
    "# Useful for git push as Github gives a warning when pushing a file > 50Mb\n",
    "!zip ks-projects-201801.zip ks-projects-201801.csv\n",
    "!rm -f ks-projects-201801.csv\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep 'ks-projects-201801.csv', the most recent dataset available.\n",
    "\n",
    "<span style=\"color:red\">Run the cell below when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/qlr': File exists\n",
      "Deleted /user/qlr/ks-projects-201801.csv\n",
      "Found 1 items\n",
      "-rw-r--r--   2 root hadoop   58030359 2021-01-02 16:51 /user/qlr/ks-projects-201801.csv\n"
     ]
    }
   ],
   "source": [
    "# Uploads the dataset to HDFS when on Google Cloud\n",
    "!hdfs dfs -mkdir /user/qlr\n",
    "!hdfs dfs -rm /user/qlr/ks-projects-201801.csv\n",
    "!hdfs dfs -put ks-projects-201801.csv /user/qlr\n",
    "!hdfs dfs -ls /user/qlr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Library Imports & Spark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer, HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import unix_timestamp, ceil, isnan, when, count, col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Swap the dataset_path variable and comment out the spark_context variable when using Google Cloud:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"ks-projects-201801.csv\" #if run on a local computer\n",
    "dataset_path = \"/user/qlr/ks-projects-201801.csv\" # if run on Google Cloud\n",
    "dataset_format = \"csv\"\n",
    "#spark_context = \"local\" #if run on a local computer\n",
    "spark_context = \"yarn\" #if run on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Starting a Spark Session & Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Comment out the following cell when when running the notebook Google Cloud as a spark session is automatically instantiated.</span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sc = SparkContext(spark_context)\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-3ea8-m.europe-west1-d.c.rising-timing-296017.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efbedb45390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #Spark UI on Google Cloud should return v2.3.4 (version), yarn (Master), PySparkShell (AppName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = (spark\n",
    "             .read\n",
    "             .format(dataset_format)\n",
    "             .options(header=True)\n",
    "             .load(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_to_keep = [\n",
    "    \"ID\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "replace_start_end_dates_with_duration = [\n",
    "    \"ID\",\"name\",\"category\",\"total_duration\",\"country\",\"usd_goal_real\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "kept_columns_for_1st_batch_of_models = [\n",
    "    \"total_duration\",\"usd_goal_real\",\"name\",\"category\",\"country\", #features\n",
    "    \"state\" # target\n",
    "]\n",
    "\n",
    "deadline_format = \"yyyy-MM-dd\"\n",
    "launched_format = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('main_category', 'string'),\n",
       " ('currency', 'string'),\n",
       " ('deadline', 'string'),\n",
       " ('goal', 'string'),\n",
       " ('launched', 'string'),\n",
       " ('pledged', 'string'),\n",
       " ('state', 'string'),\n",
       " ('backers', 'string'),\n",
       " ('country', 'string'),\n",
       " ('usd pledged', 'string'),\n",
       " ('usd_pledged_real', 'string'),\n",
       " ('usd_goal_real', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks the type of each columns\n",
    "campaigns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "| ID|name|category|main_category|currency|deadline|goal|launched|pledged|state|backers|country|usd pledged|usd_pledged_real|usd_goal_real|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "|  0|   0|       0|            0|       0|       0|   0|       0|      0|    0|      0|      0|          0|               0|            0|\n",
      "+---+----+--------+-------------+--------+--------+----+--------+-------+-----+-------+-------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drops NAs, Nulls, and Duplicates as pySpark can raise an error during .fit() procedures\n",
    "campaigns = campaigns.dropna()\n",
    "campaigns = campaigns.dropDuplicates()\n",
    "for column in campaigns.columns:\n",
    "    campaigns = campaigns.where(col(column).isNotNull())\n",
    "\n",
    "# Checks for N/A\n",
    "campaigns.select([count(when(isnan(c), c)).alias(c) for c in campaigns.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 374855 rows.\n",
      "+----------+--------------------+----------+----------+-------------------+-------+-------------+----------+\n",
      "|        ID|                name|  category|  deadline|           launched|country|usd_goal_real|     state|\n",
      "+----------+--------------------+----------+----------+-------------------+-------+-------------+----------+\n",
      "|1343448542|RWE PRESENTS: NYE...|       Pop|2013-12-02|2013-11-22 02:33:17|     US|    100000.00|  canceled|\n",
      "|1344240212|One-sided Story y...| Art Books|2013-03-18|2013-02-16 19:41:31|     US|      2000.00|successful|\n",
      "|1344521181|          Flatlander|   Fiction|2015-11-02|2015-09-16 04:44:41|     US|      3000.00|successful|\n",
      "|1346558846|Bring readline to...|Technology|2015-06-06|2015-05-22 22:33:48|     US|      5000.00|  canceled|\n",
      "|  13466532|Printing of Betha...|      Jazz|2011-10-26|2011-10-12 16:16:05|     US|      1200.00|    failed|\n",
      "+----------+--------------------+----------+----------+-------------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeps only the relevant columns\n",
    "campaigns = campaigns.select(raw_columns_to_keep)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(campaigns.count()) + \" rows.\")\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------+-------+-------------+----------+\n",
      "|        ID|                name|  category|total_duration|country|usd_goal_real|     state|\n",
      "+----------+--------------------+----------+--------------+-------+-------------+----------+\n",
      "|1343448542|RWE PRESENTS: NYE...|       Pop|            10|     US|    100000.00|  canceled|\n",
      "|1344240212|One-sided Story y...| Art Books|            30|     US|      2000.00|successful|\n",
      "|1344521181|          Flatlander|   Fiction|            47|     US|      3000.00|successful|\n",
      "|1346558846|Bring readline to...|Technology|            15|     US|      5000.00|  canceled|\n",
      "|  13466532|Printing of Betha...|      Jazz|            14|     US|      1200.00|    failed|\n",
      "+----------+--------------------+----------+--------------+-------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes a duration time (in day) from the launch and deadline features before dropping them\n",
    "launch_times = unix_timestamp('launched', format = launched_format)\n",
    "deadline_times = unix_timestamp('deadline', format = deadline_format)\n",
    "time_difference = deadline_times - launch_times\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"total_duration\",ceil(time_difference/(3600*24))).\\\n",
    "    select(replace_start_end_dates_with_duration)\n",
    "# --\n",
    "campaigns.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     state| count|\n",
      "+----------+------+\n",
      "|    failed|237451|\n",
      "|successful|134609|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleans the target labels\n",
    "# 'undefied', 'live' -> dropped\n",
    "# 'suspended', 'cancelled' -> renamed to 'failed'\n",
    "for condition in ['state!=\"undefined\"', 'state!=\"live\"']:\n",
    "    campaigns = campaigns.where(condition)\n",
    "campaigns = campaigns.\\\n",
    "    withColumn(\"state\",when(col(\"state\") == \"canceled\", \"failed\").\\\n",
    "    when(col(\"state\") == \"suspended\", \"failed\").\\\n",
    "    when(col(\"state\") == \"failed\", \"failed\").\\\n",
    "    otherwise(\"successful\"))\n",
    "campaigns.select(\"state\").groupBy('state').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casts the relevant column(s) to their end types\n",
    "for column in [\"total_duration\", \"usd_goal_real\"]:\n",
    "    campaigns = campaigns.withColumn(column,col(column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Creating the dataset for Logistic Regression, Decision Tree, and Random Forest\n",
    "\n",
    "\n",
    "We will rely on indexing and assembling our data pipeline using the following stages:\n",
    "- **StringIndexer** for all categorical columns\n",
    "- **OneHotEncoder** for all categorical index columns\n",
    "- **VectorAssembler** for all feature columns into one vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 372060 rows.\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "|total_duration|usd_goal_real|                name|      category|country|     state|\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "|          43.0|      4926.39|             Borders|         Drama|     GB|    failed|\n",
      "|          21.0|      2240.39|Spiele für iOS un...|  Mobile Games|     DE|    failed|\n",
      "|          30.0|        700.0|Odyssey Skateboar...|Graphic Design|     US|    failed|\n",
      "|          30.0|       5500.0|Debut EP Album Pr...|           R&B|     US|    failed|\n",
      "|          16.0|       1200.0|GBS Detroit Prese...|    Indie Rock|     US|successful|\n",
      "+--------------+-------------+--------------------+--------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('total_duration', 'float'),\n",
       " ('usd_goal_real', 'float'),\n",
       " ('name', 'string'),\n",
       " ('category', 'string'),\n",
       " ('country', 'string'),\n",
       " ('state', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes ID and name from the dataset\n",
    "first_batch_models_dataset = campaigns.select(kept_columns_for_1st_batch_of_models)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(first_batch_models_dataset.count()) + \" rows.\")\n",
    "first_batch_models_dataset.show(n=5)\n",
    "first_batch_models_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to string index each categorical feature column except name, and the label column\n",
    "categorical_feature_columns = first_batch_models_dataset.columns[3:]\n",
    "string_indexing_feature_columns = [StringIndexer(inputCol=column, \n",
    "                                                 outputCol='strindexed_' + column,\n",
    "                                                 handleInvalid=\"skip\")\n",
    "                                   for column in categorical_feature_columns]\n",
    "string_indexing_label_column = [StringIndexer(inputCol='state', \n",
    "                                              outputCol='label',\n",
    "                                              handleInvalid=\"skip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates pipeline stages to one-hot encode each categorical feature column except name\n",
    "if spark_context == \"local\":\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                     outputCol='onehot_' + column,\n",
    "                                                  handleInvalid=\"keep\") \n",
    "                                      for column in categorical_feature_columns]\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    onehot_encoding_feature_columns = [OneHotEncoder(inputCol='strindexed_' + column, \n",
    "                                                 outputCol='onehot_' + column) \n",
    "                                  for column in categorical_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['onehot_category', 'onehot_country', 'total_duration', 'usd_goal_real']\n"
     ]
    }
   ],
   "source": [
    "# Creates a pipeline stage to vector assemble each categorical feature column except name\n",
    "processed_feature_columns = list(map(lambda col_name: \"onehot_\" + col_name, categorical_feature_columns))\n",
    "processed_feature_columns += [\"total_duration\", \"usd_goal_real\"]\n",
    "processed_feature_columns.remove(\"onehot_state\")\n",
    "print(processed_feature_columns)\n",
    "\n",
    "if spark_context == \"local\":\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features_1',\n",
    "                                            handleInvalid=\"skip\")\n",
    "else: #Google Cloud's version of PySpark does not support/need handleInvalid attributes\n",
    "    vectorassembler_stage = VectorAssembler(inputCols=processed_feature_columns, \n",
    "                                            outputCol='features_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"words\")\n",
    "Word2Vec = Word2Vec(vectorSize=10, inputCol=tokenizer.getOutputCol(), outputCol=\"features_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_features = VectorAssembler(inputCols=[\"features_1\", \"features_2\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage] + \n",
    "    [tokenizer] + \n",
    "    [Word2Vec] +\n",
    "    [merge_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_first_batch_models = data_processing_pipeline.fit(first_batch_models_dataset.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Finalizing our dataset for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"words\")\n",
    "HashingTF = HashingTF(numFeatures=20, inputCol=tokenizer.getOutputCol(), outputCol=\"features_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembles the data processing pipeline\n",
    "data_processing_pipeline = Pipeline(\n",
    "    stages = string_indexing_feature_columns +\n",
    "    string_indexing_label_column + \n",
    "    onehot_encoding_feature_columns + \n",
    "    [vectorassembler_stage] + \n",
    "    [tokenizer] + \n",
    "    [HashingTF] +\n",
    "    [merge_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data processing pipeline\n",
    "pipeline_naive_bayes = data_processing_pipeline.fit(first_batch_models_dataset.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running a Logistic Regression Pipeline\n",
    "\n",
    "### 7.1. Finalizing the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "training_size = 0.7\n",
    "test_size = 0.3\n",
    "reg_parameters = [0, 0.5, 1, 2]\n",
    "elastic_net_parameters = [0, 0.5, 1]\n",
    "\n",
    "def process_confusion_matrix(matrix):\n",
    "    items = []\n",
    "    for item in matrix: \n",
    "        items.append(item)\n",
    "        print(item, matrix[item])\n",
    "    if Row(label=0.0, prediction=0.0) in items: \n",
    "        true_negatives = float(matrix[Row(label=0.0, prediction=0.0)])\n",
    "    else: \n",
    "        true_negatives = 0.\n",
    "    if Row(label=1.0, prediction=0.0) in items: \n",
    "        false_negatives = float(matrix[Row(label=1.0, prediction=0.0)])\n",
    "    else: \n",
    "        false_negatives = 0.\n",
    "    if Row(label=0.0, prediction=1.0) in items: \n",
    "        false_positives = float(matrix[Row(label=0.0, prediction=1.0)])\n",
    "    else: \n",
    "        false_positives = 0.\n",
    "    if Row(label=1.0, prediction=1.0) in items: \n",
    "        true_positives = float(matrix[Row(label=1.0, prediction=1.0)])\n",
    "    else: \n",
    "        true_positives = 0.\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    print(\"\\nPrecision score:\", precision)\n",
    "    print(\"Recall score:\", recall)\n",
    "    if precision+recall != 0.: print(\"F1 score:\", (precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+--------------------+--------------------+\n",
      "|          features_1|          features_2|\n",
      "+--------------------+--------------------+\n",
      "|(181,[26,158,179,...|[0.05186719766684...|\n",
      "|(181,[5,158,179,1...|[0.35906347632408...|\n",
      "|(181,[51,159,179,...|[0.03446385506540...|\n",
      "|(181,[43,158,179,...|[0.03267886023968...|\n",
      "|(181,[5,159,179,1...|[0.04297338426113...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = ['features', 'label']\n",
    "first_batch_dataset_prepped = pipeline_first_batch_models.\\\n",
    "    transform(first_batch_models_dataset)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(first_batch_dataset_prepped.count()) + \" rows.\")        \n",
    "first_batch_dataset_prepped.select([\"features_1\", \"features_2\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3711 rows.\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(191,[12,166,179,...|  0.0|\n",
      "|(191,[13,158,179,...|  1.0|\n",
      "|(191,[8,158,179,1...|  0.0|\n",
      "|(191,[3,158,179,1...|  0.0|\n",
      "|(191,[28,158,179,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_batch_dataset_prepped = first_batch_dataset_prepped.\\\n",
    "    select(final_columns).sample(0.01).cache()\n",
    "# --\n",
    "print(\"The dataset contains \" + str(first_batch_dataset_prepped.count()) + \" rows.\")        \n",
    "first_batch_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Creating a model pipeline\n",
    "\n",
    "#### 7.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0., 0.5, 1., 2.]).\\\n",
    "    addGrid(lr.elasticNetParam, [0., 0.5, 1.]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(first_batch_dataset_prepped.na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was fit using parameters: \n",
      "{Param(parent=u'CrossValidatorModel_4696830d4887abebd9de', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.5, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.5, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 0.5, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 1.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 1.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 1.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 2.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 2.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}, {Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='regParam', doc='regularization parameter (>= 0).'): 2.0, Param(parent=u'LogisticRegression_4c7c81df79b94a8909b5', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}], Param(parent=u'CrossValidatorModel_4696830d4887abebd9de', name='estimator', doc='estimator to be cross-validated'): LogisticRegression_4c7c81df79b94a8909b5, Param(parent=u'CrossValidatorModel_4696830d4887abebd9de', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): BinaryClassificationEvaluator_4547afba264998a57d27, Param(parent=u'CrossValidatorModel_4696830d4887abebd9de', name='seed', doc='random seed.'): -4372709618522015412}\n"
     ]
    }
   ],
   "source": [
    "print(\"The model was fit using parameters: \")\n",
    "print(cv_model.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(label=0.0, prediction=1.0), 349)\n",
      "(Row(label=1.0, prediction=0.0), 734)\n",
      "(Row(label=0.0, prediction=0.0), 2030)\n",
      "(Row(label=1.0, prediction=1.0), 598)\n",
      "('\\nPrecision score:', 0.6314677930306231)\n",
      "('Recall score:', 0.44894894894894893)\n",
      "('F1 score:', 0.2623957876261518)\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(first_batch_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -1.19133135782\n",
      "coefficients: [2.883169701788507,2.8822537448249506,3.4110199938860206,3.2290771491118178,3.426121102437037,2.2029277326287002,2.44258890183341,2.580245409153952,1.7058287523020317,2.001547864443571,1.8049231264298944,2.5813494120723406,1.8365524647872513,2.8891735526482836,2.8493117609947034,2.6234018841832594,2.1246119267337518,1.0552371706685628,2.2473371836537,2.290743279839454,3.7352507907286285,2.127609521157579,2.7682828412005467,0.7000294790882924,3.199848347512026,2.080313460845413,3.9510877888628766,2.88936139237162,1.103806422039922,3.091092158160054,2.854976653257445,2.202866134868444,2.807568849907514,2.7291544731502126,1.4698833233890645,3.6248033484229194,1.0719345219076695,3.3689809815080234,1.9013346599587657,1.9520510263376063,3.3816058076716704,2.9586757098005068,3.3678674099689436,2.734566754657879,1.8644788110440902,2.544353103001597,3.5042498871059053,3.3074304189772614,2.288088161295042,2.7788548311194026,2.590052752961918,3.5340334064518393,2.114321370868766,2.7879260526776557,3.0444128821494774,3.7519977012847225,2.7992351266845974,-135.21309134087403,3.019680201147453,1.7734978475699936,3.418788767906541,3.39815579908236,2.9549839044521247,1.8867407341390667,2.5389109712255387,3.1203197637733,2.2482696696734155,2.544931235305864,2.085671986792197,0.5804001402456924,2.0932713614709137,1.4208453497774443,2.789259394449321,1.9019442261797148,1.8019684629006487,-132.94402119557404,4.2344778869941075,3.8546817577737085,3.170712208699489,3.0360093221770565,-134.10203569761615,4.052230452280517,3.5785842107447605,2.420870439122513,2.939074827434689,-129.0802832206633,-158.11996072653807,3.1168734485760736,4.835713039715903,1.4492086522842138,3.760998783378196,2.8046185689854695,-134.37500551216,2.3892732666303274,1.82572778328361,2.8285976662008507,1.7977694926006924,2.760896162999458,4.643442021285416,4.1120991742264765,3.036968194033128,142.55627897728291,3.8988340153172927,2.7825490112101887,2.8124409724621158,-120.447599758479,2.2812100972869453,1.9560869662950726,1.5431529399086132,3.561128820350393,1.9280915199548527,1.6316093364182578,-143.2275640031029,2.6484161790762357,-139.73990853107784,3.290651960959177,146.98191443077857,4.351702812357143,2.6382651790073273,4.070850933660507,2.040995737681773,-104.74120118621818,3.0707363299246504,151.61706945537026,0.0,-137.66540323902032,147.68130055784462,3.597254244014199,2.3277087177774596,2.565508069867483,-130.12273733624724,-133.57951925435006,3.7953107556971433,-154.03123592165832,2.264926234007939,-129.0040800974343,144.28116280350096,-105.97849986468441,3.6935519486827415,-140.66597116134656,2.894213691559161,-129.79157723522357,145.80017009228393,-142.78775384153153,0.0,-95.99984709550915,-123.00396131717362,-142.96645169292185,2.598735957772958,-111.0813876492338,0.0,0.0,-140.93076858252837,0.0,0.0,0.0,0.0,0.0,-1.1415768097278984,-1.1201605742686005,-0.9728721034980637,-1.5742565719437691,-1.725512711408858,-1.7936748049335787,-1.40985538863147,-1.512869982936824,-2.7552643246277144,-1.2971391094395972,-1.9092769014834954,-1.006710838373876,-1.1513341893670668,-1.8541152788160642,-1.3646388440881918,-141.99647103803093,-1.1364637567885085,-0.35987596811648326,-2.225854449818618,113.19838842209487,0.0,-0.018526455024768615,-2.592512796497797e-05,-0.16983890143767855,0.1588547292112378,0.13550649229736744,-0.22158880726024516,-0.1503933353580051,0.42958985314369996,0.25902000653388113,-1.0495712619903277,1.013895613137988,0.26737873779133187]\n"
     ]
    }
   ],
   "source": [
    "# Intercept and Coefficients of the regresison model\n",
    "print('Intercept: ' + str(cv_model.bestModel.intercept) + \"\\n\"\n",
    "      'coefficients: ' + str(cv_model.bestModel.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The best RegParam is: ', 0.0, '\\n', 'The best ElasticNetParam is: cv_model.bestModel._java_obj.getElasticNetParam()')\n"
     ]
    }
   ],
   "source": [
    "# Parameters of the best model\n",
    "print('The best RegParam is: ', cv_model.bestModel._java_obj.getRegParam(), \"\\n\",\n",
    "     'The best ElasticNetParam is: cv_model.bestModel._java_obj.getElasticNetParam()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running a DecisionTreeClassifier Pipeline\n",
    "\n",
    "### 8.1. Finalizing the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "max_depth_grid = list(range(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset between training and validation sets\n",
    "training, test = first_batch_dataset_prepped.randomSplit([training_size, test_size], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 8.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_crossvalidation = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(decision_tree_with_crossvalidation.maxDepth, max_depth_grid).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=decision_tree_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(first_batch_dataset_prepped.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------+--------------------+\n",
      "|            features|label|prediction| rawPrediction|         probability|\n",
      "+--------------------+-----+----------+--------------+--------------------+\n",
      "|(191,[12,166,179,...|  0.0|       0.0| [840.0,324.0]|[0.72164948453608...|\n",
      "|(191,[13,158,179,...|  1.0|       0.0|[1089.0,850.0]|[0.56162970603403...|\n",
      "|(191,[8,158,179,1...|  0.0|       0.0|[1089.0,850.0]|[0.56162970603403...|\n",
      "|(191,[3,158,179,1...|  0.0|       0.0| [840.0,324.0]|[0.72164948453608...|\n",
      "|(191,[28,158,179,...|  0.0|       0.0|[1089.0,850.0]|[0.56162970603403...|\n",
      "+--------------------+-----+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(first_batch_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(label=0.0, prediction=1.0), 52)\n",
      "(Row(label=1.0, prediction=0.0), 1229)\n",
      "(Row(label=0.0, prediction=0.0), 2327)\n",
      "(Row(label=1.0, prediction=1.0), 103)\n",
      "('\\nPrecision score:', 0.6645161290322581)\n",
      "('Recall score:', 0.07732732732732733)\n",
      "('F1 score:', 0.06926698049764626)\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(first_batch_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The best MaxDepth is:', 2)\n"
     ]
    }
   ],
   "source": [
    "print('The best MaxDepth is:', cv_model.bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_489a81d9127568ddb644) of depth 2 with 7 nodes\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Creating a model pipeline using Train-Test split\n",
    "\n",
    "#### 8.3.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "decision_tree_with_traintestsplit = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "traintest_model = decision_tree_with_traintestsplit.fit(training.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+-------------+--------------------+\n",
      "|            features|label|prediction|rawPrediction|         probability|\n",
      "+--------------------+-----+----------+-------------+--------------------+\n",
      "|(191,[3,158,179,1...|  0.0|       0.0|[520.0,224.0]|[0.69892473118279...|\n",
      "|(191,[8,158,179,1...|  0.0|       0.0|[233.0,122.0]|[0.65633802816901...|\n",
      "|(191,[9,158,179,1...|  1.0|       1.0|[168.0,230.0]|[0.42211055276381...|\n",
      "|(191,[9,158,179,1...|  0.0|       0.0|[520.0,224.0]|[0.69892473118279...|\n",
      "|(191,[13,158,179,...|  1.0|       0.0|  [60.0,56.0]|[0.51724137931034...|\n",
      "+--------------------+-----+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_test = traintest_model.transform(test)\n",
    "pred_test.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The test error is', 0.462692312887496)\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(\"The test error is\", 1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4083bc918fe7e60ac47a) of depth 5 with 57 nodes\n"
     ]
    }
   ],
   "source": [
    "print(traintest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running a Random Forest Pipeline\n",
    "\n",
    "### 9.1. Finalizing the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "max_depth_grid = list(range(2,10))\n",
    "minimum_info_grain = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 9.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "random_forest_with_crossvalidation = RandomForestClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(random_forest_with_crossvalidation.maxDepth, max_depth_grid).\\\n",
    "    addGrid(random_forest_with_crossvalidation.minInfoGain, minimum_info_grain).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=random_forest_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(first_batch_dataset_prepped.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|            features|label|prediction|       rawPrediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|(191,[12,166,179,...|  0.0|       0.0|[15.4419633230080...|[0.77209816615040...|\n",
      "|(191,[13,158,179,...|  1.0|       0.0|[11.9675484158580...|[0.59837742079290...|\n",
      "|(191,[8,158,179,1...|  0.0|       0.0|[14.2468879425841...|[0.71234439712920...|\n",
      "|(191,[3,158,179,1...|  0.0|       0.0|[12.6286258514609...|[0.63143129257304...|\n",
      "|(191,[28,158,179,...|  0.0|       0.0|[12.5537399612295...|[0.62768699806147...|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(first_batch_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(label=0.0, prediction=1.0), 53)\n",
      "(Row(label=1.0, prediction=0.0), 1015)\n",
      "(Row(label=0.0, prediction=0.0), 2326)\n",
      "(Row(label=1.0, prediction=1.0), 317)\n",
      "('\\nPrecision score:', 0.8567567567567568)\n",
      "('Recall score:', 0.23798798798798798)\n",
      "('F1 score:', 0.18625146886016453)\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(first_batch_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=RandomForestClassifier_478dbc6cf5e34379607c) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Running a Naive Bayes Pipeline\n",
    "\n",
    "### 10.1. Finalizing the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares hyperparameters\n",
    "smoothing = map(float, list(range(0,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 370775 rows.\n",
      "+--------------------+--------------------+\n",
      "|          features_1|          features_2|\n",
      "+--------------------+--------------------+\n",
      "|(181,[49,159,179,...|      (20,[9],[1.0])|\n",
      "|(181,[57,162,179,...|(20,[2,7,8,9,11,1...|\n",
      "|(181,[52,158,179,...|(20,[1,3,18],[1.0...|\n",
      "|(181,[107,158,179...|(20,[1,4,8,15],[1...|\n",
      "|(181,[20,158,179,...|(20,[1,9,10,12,13...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_columns = ['features', 'label']\n",
    "naive_bayes_dataset_prepped = pipeline_naive_bayes.\\\n",
    "    transform(first_batch_models_dataset)\n",
    "# --\n",
    "print(\"The dataset contains \" + str(naive_bayes_dataset_prepped.count()) + \" rows.\")        \n",
    "naive_bayes_dataset_prepped.select([\"features_1\", \"features_2\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3812 rows.\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(201,[11,161,179,...|  1.0|\n",
      "|(201,[15,158,179,...|  1.0|\n",
      "|(201,[1,158,179,1...|  1.0|\n",
      "|(201,[40,163,179,...|  0.0|\n",
      "|(201,[9,158,179,1...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_dataset_prepped = naive_bayes_dataset_prepped.\\\n",
    "    select(final_columns).sample(0.01).cache()\n",
    "# --\n",
    "print(\"The dataset contains \" + str(naive_bayes_dataset_prepped.count()) + \" rows.\")        \n",
    "naive_bayes_dataset_prepped.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Creating a model pipeline using cross-validation\n",
    "\n",
    "#### 10.2.1. Building and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the estimator\n",
    "naive_bayes_with_crossvalidation = NaiveBayes(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(naive_bayes_with_crossvalidation.smoothing, smoothing).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the cross-validation model\n",
    "cv = CrossValidator(estimator=naive_bayes_with_crossvalidation, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fits the cross-validation model\n",
    "cv_model = cv.fit(naive_bayes_dataset_prepped.na.drop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.2. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|            features|label|prediction|       rawPrediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "|(201,[11,161,179,...|  1.0|       1.0|[-295.18347220871...|[8.58502538406355...|\n",
      "|(201,[15,158,179,...|  1.0|       1.0|[-331.33323159063...|[4.46878495662525...|\n",
      "|(201,[1,158,179,1...|  1.0|       1.0|[-392.11114538793...|[2.46258657583464...|\n",
      "|(201,[40,163,179,...|  0.0|       1.0|[-408.81133075316...|[9.08339036052063...|\n",
      "|(201,[9,158,179,1...|  0.0|       0.0|[-333.11701353272...|[0.99991585927173...|\n",
      "+--------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicts on training data\n",
    "show_columns = ['features', 'label', 'prediction', 'rawPrediction', 'probability']\n",
    "pred_training_cv = cv_model.transform(naive_bayes_dataset_prepped)\n",
    "pred_training_cv.select(show_columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(label=0.0, prediction=1.0), 1683)\n",
      "(Row(label=1.0, prediction=0.0), 172)\n",
      "(Row(label=0.0, prediction=0.0), 755)\n",
      "(Row(label=1.0, prediction=1.0), 1202)\n",
      "('\\nPrecision score:', 0.41663778162911613)\n",
      "('Recall score:', 0.8748180494905385)\n",
      "('F1 score:', 0.28222587461845505)\n"
     ]
    }
   ],
   "source": [
    "# Provides a confusion matrix\n",
    "label_and_pred = cv_model.transform(naive_bayes_dataset_prepped).select('label', 'prediction')\n",
    "confusion_matrix = label_and_pred.rdd.zipWithIndex().countByKey()\n",
    "process_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes_474fb50cda2c3a8aff1b\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
